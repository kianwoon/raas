{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Veritas Fairness Assessment\n",
    "\n",
    "This notebook performs comprehensive fairness assessment on ML models using the Veritas framework.\n",
    "\n",
    "## Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "import logging\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "\n",
    "# AI Fairness libraries\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from aif360.algorithms.preprocessing import Reweighing, DisparateImpactRemover\n",
    "from aif360.algorithms.inprocessing import AdversarialDebiasing, PrejudiceRemover\n",
    "from aif360.algorithms.postprocessing import CalibratedEqOddsPostprocessing, EqOddsPostprocessing\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration from job parameters\n",
    "config = {\n",
    "    \"assessment_id\": \"${assessment_id}\",\n",
    "    \"model_name\": \"${model_name}\",\n",
    "    \"model_version\": \"${model_version}\",\n",
    "    \"target_column\": \"${target_column}\",\n",
    "    \"protected_attributes\": ${protected_attributes},\n",
    "    \"fairness_thresholds\": ${fairness_thresholds},\n",
    "    \"data_source_id\": \"${data_source_id}\",\n",
    "    \"data_query\": \"${data_query}\",\n",
    "    \"test_size\": ${test_size},\n",
    "    \"random_seed\": ${random_seed},\n",
    "    \"confidence_level\": ${confidence_level}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data() -> pd.DataFrame:\n",
    "    \"\"\"Load data based on configuration.\"\"\"\n",
    "    try:\n",
    "        # If data source is provided, load from database\n",
    "        if config.get('data_source_id'):\n",
    "            # TODO: Implement database connection based on data source\n",
    "            # For now, return sample data\n",
    "            logger.info(f\"Loading data from source: {config['data_source_id']}\")\n",
    "            return load_sample_data()\n",
    "        \n",
    "        # If data query is provided, execute it\n",
    "        elif config.get('data_query'):\n",
    "            logger.info(\"Executing data query\")\n",
    "            # TODO: Implement query execution\n",
    "            return load_sample_data()\n",
    "        \n",
    "        else:\n",
    "            logger.info(\"Using sample data\")\n",
    "            return load_sample_data()\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def load_sample_data() -> pd.DataFrame:\n",
    "    \"\"\"Load sample fairness assessment data.\"\"\"\n",
    "    np.random.seed(config.get('random_seed', 42))\n",
    "    n_samples = 10000\n",
    "    \n",
    "    # Generate synthetic data\n",
    "    data = {\n",
    "        'age': np.random.normal(45, 15, n_samples).astype(int),\n",
    "        'gender': np.random.choice(['Male', 'Female'], n_samples, p=[0.6, 0.4]),\n",
    "        'race': np.random.choice(['White', 'Black', 'Asian', 'Hispanic'], n_samples, p=[0.5, 0.2, 0.15, 0.15]),\n",
    "        'income': np.random.normal(50000, 20000, n_samples),\n",
    "        'credit_score': np.random.normal(650, 100, n_samples).astype(int),\n",
    "        'loan_amount': np.random.normal(200000, 80000, n_samples),\n",
    "        'employment_years': np.random.exponential(5, n_samples).astype(int)\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Create target variable with some bias\n",
    "    # Base probability\n",
    "    base_prob = 0.7\n",
    "    \n",
    "    # Add bias factors\n",
    "    bias_factors = {\n",
    "        'Male': 1.1,  # 10% advantage\n",
    "        'Female': 0.9,  # 10% disadvantage\n",
    "        'White': 1.15,  # 15% advantage\n",
    "        'Black': 0.85,  # 15% disadvantage\n",
    "        'Asian': 1.05,  # 5% advantage\n",
    "        'Hispanic': 0.9  # 10% disadvantage\n",
    "    }\n",
    "    \n",
    "    probabilities = []\n",
    "    for _, row in df.iterrows():\n",
    "        prob = base_prob\n",
    "        \n",
    "        # Apply gender bias\n",
    "        prob *= bias_factors.get(row['gender'], 1.0)\n",
    "        \n",
    "        # Apply race bias\n",
    "        prob *= bias_factors.get(row['race'], 1.0)\n",
    "        \n",
    "        # Add some noise\n",
    "        prob += np.random.normal(0, 0.1)\n",
    "        \n",
    "        # Ensure probability is between 0 and 1\n",
    "        prob = np.clip(prob, 0, 1)\n",
    "        \n",
    "        probabilities.append(prob)\n",
    "    \n",
    "    # Generate target variable\n",
    "    df[config['target_column']] = (np.random.random(n_samples) < probabilities).astype(int)\n",
    "    \n",
    "    # Apply some business rules\n",
    "    df.loc[df['credit_score'] < 580, config['target_column']] = 0  # Poor credit\n",
    "    df.loc[df['income'] < 20000, config['target_column']] = 0  # Low income\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the data\n",
    "df = load_data()\n",
    "logger.info(f\"Loaded {len(df)} samples\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "    \"\"\"Preprocess data for fairness assessment.\"\"\"\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    df_processed = df_processed.dropna()\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    label_encoders = {}\n",
    "    for attr in config['protected_attributes']:\n",
    "        attr_name = attr['name']\n",
    "        if attr_name in df_processed.columns:\n",
    "            le = LabelEncoder()\n",
    "            df_processed[f\"{attr_name}_encoded\"] = le.fit_transform(df_processed[attr_name])\n",
    "            label_encoders[attr_name] = le\n",
    "    \n",
    "    # Create binary features for protected attributes\n",
    "    for attr in config['protected_attributes']:\n",
    "        attr_name = attr['name']\n",
    "        privileged_groups = attr['privileged_groups']\n",
    "        \n",
    "        if attr_name in df_processed.columns:\n",
    "            df_processed[f\"{attr_name}_privileged\"] = df_processed[attr_name].isin(privileged_groups).astype(int)\n",
    "    \n",
    "    return df_processed, label_encoders\n",
    "\n",
    "# Preprocess the data\n",
    "df_processed, label_encoders = preprocess_data(df)\n",
    "logger.info(f\"Preprocessed {len(df_processed)} samples\")\n",
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Training and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train: pd.DataFrame, y_train: pd.Series) -> Any:\n",
    "    \"\"\"Train a model for fairness assessment.\"\"\"\n",
    "    # Select features (excluding protected attributes and target)\n",
    "    feature_cols = [col for col in X_train.columns \n",
    "                    if not col.endswith('_encoded') \n",
    "                    and not col.endswith('_privileged')\n",
    "                    and col not in [attr['name'] for attr in config['protected_attributes']]\n",
    "                    and col != config['target_column']]\n",
    "    \n",
    "    X_train_features = X_train[feature_cols]\n",
    "    \n",
    "    # Scale numerical features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_features)\n",
    "    \n",
    "    # Train model\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=config.get('random_seed', 42))\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    return model, scaler, feature_cols\n",
    "\n",
    "def make_predictions(model: Any, scaler: Any, feature_cols: List[str], X: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"Make predictions using the trained model.\"\"\"\n",
    "    X_features = X[feature_cols]\n",
    "    X_scaled = scaler.transform(X_features)\n",
    "    \n",
    "    predictions = model.predict(X_scaled)\n",
    "    prediction_probs = model.predict_proba(X_scaled)[:, 1]\n",
    "    \n",
    "    return predictions, prediction_probs\n",
    "\n",
    "# Split data\n",
    "X = df_processed.drop(columns=[config['target_column']])\n",
    "y = df_processed[config['target_column']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=config['test_size'], \n",
    "    random_state=config.get('random_seed', 42),\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Train model\n",
    "model, scaler, feature_cols = train_model(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred, y_pred_proba = make_predictions(model, scaler, feature_cols, X_test)\n",
    "\n",
    "logger.info(f\"Model accuracy: {accuracy_score(y_test, y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fairness Metrics Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fairness_metrics(df_test: pd.DataFrame, y_test: pd.Series, y_pred: np.ndarray, y_pred_proba: np.ndarray) -> Dict[str, Any]:\n",
    "    \"\"\"Calculate comprehensive fairness metrics.\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    for attr in config['protected_attributes']:\n",
    "        attr_name = attr['name']\n",
    "        privileged_groups = attr['privileged_groups']\n",
    "        unprivileged_groups = attr['unprivileged_groups']\n",
    "        \n",
    "        logger.info(f\"Calculating fairness metrics for {attr_name}\")\n",
    "        \n",
    "        # Create AIF360 dataset\n",
    "        df_aif = df_test.copy()\n",
    "        df_aif['scores'] = y_pred_proba\n",
    "        df_aif['labels'] = y_pred\n",
    "        \n",
    "        # Define privileged and unprivileged groups\n",
    "        privileged_mask = df_aif[attr_name].isin(privileged_groups)\n",
    "        unprivileged_mask = df_aif[attr_name].isin(unprivileged_groups)\n",
    "        \n",
    "        # Calculate basic metrics\n",
    "        metrics[attr_name] = calculate_group_metrics(\n",
    "            y_test[privileged_mask], y_pred[privileged_mask],\n",
    "            y_test[unprivileged_mask], y_pred[unprivileged_mask],\n",
    "            attr_name, privileged_groups[0], unprivileged_groups[0]\n",
    "        )\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def calculate_group_metrics(y_priv: pd.Series, y_pred_priv: pd.Series, \n",
    "                          y_unpriv: pd.Series, y_pred_unpriv: pd.Series,\n",
    "                          attr_name: str, priv_value: str, unpriv_value: str) -> Dict[str, Any]:\n",
    "    \"\"\"Calculate fairness metrics for privileged and unprivileged groups.\"\"\"\n",
    "    # Basic metrics\n",
    "    priv_metrics = {\n",
    "        'selection_rate': np.mean(y_pred_priv),\n",
    "        'accuracy': accuracy_score(y_priv, y_pred_priv),\n",
    "        'precision': precision_score(y_priv, y_pred_priv, zero_division=0),\n",
    "        'recall': recall_score(y_priv, y_pred_priv, zero_division=0),\n",
    "        'f1_score': f1_score(y_priv, y_pred_priv, zero_division=0)\n",
    "    }\n",
    "    \n",
    "    unpriv_metrics = {\n",
    "        'selection_rate': np.mean(y_pred_unpriv),\n",
    "        'accuracy': accuracy_score(y_unpriv, y_pred_unpriv),\n",
    "        'precision': precision_score(y_unpriv, y_pred_unpriv, zero_division=0),\n",
    "        'recall': recall_score(y_unpriv, y_pred_unpriv, zero_division=0),\n",
    "        'f1_score': f1_score(y_unpriv, y_pred_unpriv, zero_division=0)\n",
    "    }\n",
    "    \n",
    "    # Fairness metrics\n",
    "    demographic_parity_diff = abs(priv_metrics['selection_rate'] - unpriv_metrics['selection_rate'])\n",
    "    demographic_parity_ratio = unpriv_metrics['selection_rate'] / priv_metrics['selection_rate'] if priv_metrics['selection_rate'] > 0 else 0\n",
    "    \n",
    "    equal_opportunity_diff = abs(priv_metrics['recall'] - unpriv_metrics['recall'])\n",
    "    equal_opportunity_ratio = unpriv_metrics['recall'] / priv_metrics['recall'] if priv_metrics['recall'] > 0 else 0\n",
    "    \n",
    "    predictive_parity_diff = abs(priv_metrics['precision'] - unpriv_metrics['precision'])\n",
    "    predictive_parity_ratio = unpriv_metrics['precision'] / priv_metrics['precision'] if priv_metrics['precision'] > 0 else 0\n",
    "    \n",
    "    accuracy_parity_diff = abs(priv_metrics['accuracy'] - unpriv_metrics['accuracy'])\n",
    "    accuracy_parity_ratio = unpriv_metrics['accuracy'] / priv_metrics['accuracy'] if priv_metrics['accuracy'] > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'attribute': attr_name,\n",
    "        'privileged_value': priv_value,\n",
    "        'unprivileged_value': unpriv_value,\n",
    "        'privileged_metrics': priv_metrics,\n",
    "        'unprivileged_metrics': unpriv_metrics,\n",
    "        'fairness_metrics': {\n",
    "            'demographic_parity_difference': demographic_parity_diff,\n",
    "            'demographic_parity_ratio': demographic_parity_ratio,\n",
    "            'equal_opportunity_difference': equal_opportunity_diff,\n",
    "            'equal_opportunity_ratio': equal_opportunity_ratio,\n",
    "            'predictive_parity_difference': predictive_parity_diff,\n",
    "            'predictive_parity_ratio': predictive_parity_ratio,\n",
    "            'accuracy_parity_difference': accuracy_parity_diff,\n",
    "            'accuracy_parity_ratio': accuracy_parity_ratio\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Calculate fairness metrics\n",
    "df_test = X_test.copy()\n",
    "df_test[config['target_column']] = y_test\n",
    "\n",
    "fairness_metrics = calculate_fairness_metrics(df_test, y_test, y_pred, y_pred_proba)\n",
    "logger.info(f\"Calculated fairness metrics for {len(fairness_metrics)} protected attributes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Threshold Evaluation and Pass/Fail Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_thresholds(fairness_metrics: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Evaluate fairness metrics against thresholds.\"\"\"\n",
    "    results = {\n",
    "        'overall_fairness_score': 0.0,\n",
    "        'metric_results': [],\n",
    "        'passed_metrics': 0,\n",
    "        'failed_metrics': 0,\n",
    "        'threshold_violations': []\n",
    "    }\n",
    "    \n",
    "    metric_scores = []\n",
    "    \n",
    "    for attr_name, attr_metrics in fairness_metrics.items():\n",
    "        fairness_attr_metrics = attr_metrics['fairness_metrics']\n",
    "        \n",
    "        # Find thresholds for this attribute\n",
    "        attr_thresholds = [t for t in config['fairness_thresholds'] \n",
    "                          if t.get('protected_attribute') == attr_name or not t.get('protected_attribute')]\n",
    "        \n",
    "        for metric_name, metric_value in fairness_attr_metrics.items():\n",
    "            # Find applicable threshold\n",
    "            threshold = None\n",
    "            for t in attr_thresholds:\n",
    "                if metric_name.replace('_', '_') in t['metric_type']:\n",
    "                    threshold = t\n",
    "                    break\n",
    "            \n",
    "            if threshold:\n",
    "                # Determine if metric passes threshold\n",
    "                if threshold['threshold_type'] == 'absolute':\n",
    "                    if threshold['direction'] == 'less_than':\n",
    "                        passed = metric_value <= threshold['threshold_value']\n",
    "                    else:\n",
    "                        passed = metric_value >= threshold['threshold_value']\n",
    "                else:  # relative\n",
    "                    passed = metric_value >= threshold['threshold_value']\n",
    "                \n",
    "                # Calculate metric score (inverse of difference for difference metrics)\n",
    "                if 'difference' in metric_name:\n",
    "                    metric_score = max(0, 1 - metric_value)\n",
    "                else:  # ratio metrics\n",
    "                    metric_score = min(1, metric_value)\n",
    "                \n",
    "                metric_result = {\n",
    "                    'metric_name': metric_name,\n",
    "                    'protected_attribute': attr_name,\n",
    "                    'metric_value': metric_value,\n",
    "                    'threshold_value': threshold['threshold_value'],\n",
    "                    'threshold_type': threshold['threshold_type'],\n",
    "                    'passed': passed,\n",
    "                    'metric_score': metric_score,\n",
    "                    'privileged_value': attr_metrics['privileged_value'],\n",
    "                    'unprivileged_value': attr_metrics['unprivileged_value'],\n",
    "                    'privileged_metric': attr_metrics['privileged_metrics'],\n",
    "                    'unprivileged_metric': attr_metrics['unprivileged_metrics']\n",
    "                }\n",
    "                \n",
    "                results['metric_results'].append(metric_result)\n",
    "                metric_scores.append(metric_score)\n",
    "                \n",
    "                if passed:\n",
    "                    results['passed_metrics'] += 1\n",
    "                else:\n",
    "                    results['failed_metrics'] += 1\n",
    "                    results['threshold_violations'].append({\n",
    "                        'metric': metric_name,\n",
    "                        'attribute': attr_name,\n",
    "                        'value': metric_value,\n",
    "                        'threshold': threshold['threshold_value']\n",
    "                    })\n",
    "    \n",
    "    # Calculate overall fairness score\n",
    "    if metric_scores:\n",
    "        results['overall_fairness_score'] = np.mean(metric_scores)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate thresholds\n",
    "evaluation_results = evaluate_thresholds(fairness_metrics)\n",
    "logger.info(f\"Overall fairness score: {evaluation_results['overall_fairness_score']:.3f}\")\n",
    "logger.info(f\"Passed metrics: {evaluation_results['passed_metrics']}/{len(evaluation_results['metric_results'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization and Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_visualizations(fairness_metrics: Dict[str, Any], evaluation_results: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Create fairness assessment visualizations.\"\"\"\n",
    "    visualizations = {}\n",
    "    \n",
    "    # 1. Pass/Fail distribution\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    labels = ['Passed', 'Failed']\n",
    "    sizes = [evaluation_results['passed_metrics'], evaluation_results['failed_metrics']]\n",
    "    colors = ['#10B981', '#EF4444']\n",
    "    \n",
    "    plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "    plt.title('Fairness Metrics Pass/Fail Distribution')\n",
    "    plt.axis('equal')\n",
    "    \n",
    "    # Save plot\n",
    "    plt.savefig('/tmp/pass_fail_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    visualizations['pass_fail_chart'] = '/tmp/pass_fail_distribution.png'\n",
    "    \n",
    "    # 2. Fairness metrics by attribute\n",
    "    metrics_by_attr = {}\n",
    "    for result in evaluation_results['metric_results']:\n",
    "        attr = result['protected_attribute']\n",
    "        if attr not in metrics_by_attr:\n",
    "            metrics_by_attr[attr] = []\n",
    "        metrics_by_attr[attr].append(result['metric_score'])\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    x = np.arange(len(metrics_by_attr))\n",
    "    width = 0.35\n",
    "    \n",
    "    scores = [np.mean(metrics) for metrics in metrics_by_attr.values()]\n",
    "    bars = plt.bar(x, scores, width, alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('Protected Attributes')\n",
    "    plt.ylabel('Average Fairness Score')\n",
    "    plt.title('Fairness Scores by Protected Attribute')\n",
    "    plt.xticks(x, list(metrics_by_attr.keys()))\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, score in zip(bars, scores):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{score:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/tmp/fairness_scores_by_attr.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    visualizations['fairness_scores_by_attr'] = '/tmp/fairness_scores_by_attr.png'\n",
    "    \n",
    "    # 3. Detailed metrics comparison\n",
    "    if evaluation_results['metric_results']:\n",
    "        df_metrics = pd.DataFrame(evaluation_results['metric_results'])\n",
    "        \n",
    "        plt.figure(figsize=(14, 8))\n",
    "        metric_names = [m['metric_name'] for m in evaluation_results['metric_results'][:10]]  # Limit to 10 for readability\n",
    "        metric_values = [m['metric_value'] for m in evaluation_results['metric_results'][:10]]\n",
    "        threshold_values = [m['threshold_value'] for m in evaluation_results['metric_results'][:10]]\n",
    "        \n",
    "        x = np.arange(len(metric_names))\n",
    "        width = 0.35\n",
    "        \n",
    "        bars1 = plt.bar(x - width/2, metric_values, width, label='Actual Value', alpha=0.8)\n",
    "        bars2 = plt.bar(x + width/2, threshold_values, width, label='Threshold', alpha=0.8)\n",
    "        \n",
    "        plt.xlabel('Fairness Metrics')\n",
    "        plt.ylabel('Values')\n",
    "        plt.title('Fairness Metrics vs Thresholds')\n",
    "        plt.xticks(x, metric_names, rotation=45, ha='right')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('/tmp/metrics_vs_thresholds.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        visualizations['metrics_vs_thresholds'] = '/tmp/metrics_vs_thresholds.png'\n",
    "    \n",
    "    return visualizations\n",
    "\n",
    "# Create visualizations\n",
    "visualizations = create_visualizations(fairness_metrics, evaluation_results)\n",
    "logger.info(f\"Created {len(visualizations)} visualizations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_results_summary(fairness_metrics: Dict[str, Any], evaluation_results: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Generate comprehensive results summary.\"\"\"\n",
    "    summary = {\n",
    "        'assessment_metadata': {\n",
    "            'model_name': config['model_name'],\n",
    "            'model_version': config['model_version'],\n",
    "            'assessment_id': config['assessment_id'],\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'total_metrics': len(evaluation_results['metric_results']),\n",
    "            'overall_fairness_score': evaluation_results['overall_fairness_score']\n",
    "        },\n",
    "        'summary_statistics': {\n",
    "            'passed_metrics': evaluation_results['passed_metrics'],\n",
    "            'failed_metrics': evaluation_results['failed_metrics'],\n",
    "            'pass_rate': evaluation_results['passed_metrics'] / len(evaluation_results['metric_results']) if evaluation_results['metric_results'] else 0,\n",
    "            'average_fairness_score': evaluation_results['overall_fairness_score']\n",
    "        },\n",
    "        'detailed_metrics': evaluation_results['metric_results'],\n",
    "        'threshold_violations': evaluation_results['threshold_violations'],\n",
    "        'recommendations': generate_recommendations(evaluation_results),\n",
    "        'risk_assessment': {\n",
    "            'risk_level': calculate_risk_level(evaluation_results['overall_fairness_score']),\n",
    "            'factors': identify_risk_factors(evaluation_results)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def generate_recommendations(evaluation_results: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Generate recommendations based on assessment results.\"\"\"\n    "    recommendations = []\n",
    "    \n",
    "    # Analyze failed metrics\n",
    "    failed_by_type = {}\n",
    "    for result in evaluation_results['metric_results']:\n",
    "        if not result['passed']:\n",
    "            metric_type = result['metric_name']\n",
    "            if metric_type not in failed_by_type:\n",
    "                failed_by_type[metric_type] = []\n",
    "            failed_by_type[metric_type].append(result)\n",
    "    \n",
    "    # Generate specific recommendations\n",
    "    if 'demographic_parity_difference' in failed_by_type:\n",
    "        recommendations.append({\n",
    "            'type': 'bias_mitigation',\n",
    "            'priority': 'high',\n",
    "            'title': 'Address Demographic Parity Issues',\n",
    "            'description': 'Consider re-sampling techniques or bias mitigation algorithms to balance selection rates across groups.'\n",
    "        })\n",
    "    \n",
    "    if 'equal_opportunity_difference' in failed_by_type:\n",
    "        recommendations.append({\n",
    "            'type': 'model_adjustment',\n",
    "            'priority': 'high',\n",
    "            'title': 'Improve Equal Opportunity',\n",
    "            'description': 'Adjust model thresholds or use fairness-aware learning algorithms to improve true positive rates across groups.'\n",
    "        })\n",
    "    \n",
    "    if 'predictive_parity_difference' in failed_by_type:\n",
    "        recommendations.append({\n",
    "            'type': 'calibration',\n",
    "            'priority': 'medium',\n",
    "            'title': 'Calibration Needed',\n",
    "            'description': 'Consider calibrating model predictions to ensure consistent precision across different demographic groups.'\n",
    "        })\n",
    "    \n",
    "    if evaluation_results['overall_fairness_score'] < 0.7:\n",
    "        recommendations.append({\n",
    "            'type': 'comprehensive_review',\n",
    "            'priority': 'high',\n",
    "            'title': 'Comprehensive Model Review',\n",
    "            'description': 'The model shows significant fairness issues. Consider a comprehensive review of data, features, and model architecture.'\n",
    "        })\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "def calculate_risk_level(fairness_score: float) -> str:\n",
    "    \"\"\"Calculate risk level based on fairness score.\"\"\"\n",
    "    if fairness_score >= 0.9:\n",
    "        return 'low'\n",
    "    elif fairness_score >= 0.7:\n",
    "        return 'medium'\n",
    "    else:\n",
    "        return 'high'\n",
    "\n",
    "def identify_risk_factors(evaluation_results: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"Identify key risk factors.\"\"\"\n",
    "    risk_factors = []\n",
    "    \n",
    "    # Count violations by metric type\n",
    "    violation_counts = {}\n",
    "    for violation in evaluation_results['threshold_violations']:\n",
    "        metric_type = violation['metric']\n",
    "        violation_counts[metric_type] = violation_counts.get(metric_type, 0) + 1\n",
    "    \n",
    "    # Identify high-risk factors\n",
    "    if violation_counts.get('demographic_parity_difference', 0) > 0:\n",
    "        risk_factors.append('Demographic parity violations detected')\n",
    "    \n",
    "    if violation_counts.get('equal_opportunity_difference', 0) > 0:\n",
    "        risk_factors.append('Equal opportunity violations detected')\n",
    "    \n",
    "    if evaluation_results['failed_metrics'] > len(evaluation_results['metric_results']) * 0.3:\n",
    "        risk_factors.append('High failure rate across multiple metrics')\n",
    "    \n",
    "    return risk_factors\n",
    "\n",
    "# Generate results summary\n",
    "results_summary = generate_results_summary(fairness_metrics, evaluation_results)\n",
    "logger.info(\"Generated comprehensive results summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results and Generate Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(results_summary: Dict[str, Any], visualizations: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Save assessment results and generate report.\"\"\"\n",
    "    # Save results to JSON\n",
    "    results_file = f'/tmp/fairness_assessment_results_{config[\"assessment_id\"]}.json'\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(results_summary, f, indent=2, default=str)\n",
    "    \n",
    "    # Save visualizations\n",
    "    visualization_files = {}\n",
    "    for viz_name, viz_path in visualizations.items():\n",
    "        import os\n",
    "        if os.path.exists(viz_path):\n",
    "            new_path = f'/tmp/{viz_name}_{config[\"assessment_id\"]}.png'\n",
    "            os.rename(viz_path, new_path)\n",
    "            visualization_files[viz_name] = new_path\n",
    "    \n",
    "    # Create report metadata\n",
    "    report_metadata = {\n",
    "        'assessment_id': config['assessment_id'],\n",
    "        'model_name': config['model_name'],\n",
    "        'overall_fairness_score': results_summary['assessment_metadata']['overall_fairness_score'],\n",
    "        'pass_rate': results_summary['summary_statistics']['pass_rate'],\n",
    "        'total_metrics': results_summary['assessment_metadata']['total_metrics'],\n",
    "        'risk_level': results_summary['risk_assessment']['risk_level'],\n",
    "        'results_file': results_file,\n",
    "        'visualization_files': visualization_files,\n",
    "        'generated_at': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Save report metadata\n",
    "    metadata_file = f'/tmp/fairness_assessment_metadata_{config[\"assessment_id\"]}.json'\n",
    "    with open(metadata_file, 'w') as f:\n",
    "        json.dump(report_metadata, f, indent=2, default=str)\n",
    "    \n",
    "    return {\n",
    "        'results_file': results_file,\n",
    '        'metadata_file': metadata_file,\n",
    "        'visualization_files': visualization_files\n",
    "    }\n",
    "\n",
    "# Save results\n",
    "saved_files = save_results(results_summary, visualizations)\n",
    "logger.info(f\"Saved assessment results to {len(saved_files)} files\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FAIRNESS ASSESSMENT SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Model: {config['model_name']} (v{config['model_version']})\")\n",
    "print(f\"Overall Fairness Score: {results_summary['assessment_metadata']['overall_fairness_score']:.3f}\")\n",
    "print(f\"Pass Rate: {results_summary['summary_statistics']['pass_rate']:.1%}\")\n",
    "print(f\"Risk Level: {results_summary['risk_assessment']['risk_level'].upper()}\")\n",
    "print(f\"Metrics Passed: {results_summary['summary_statistics']['passed_metrics']}/{results_summary['assessment_metadata']['total_metrics']}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Print recommendations\n",
    "if results_summary['recommendations']:\n",
    "    print(\"\\nRECOMMENDATIONS:\")\n",
    "    for i, rec in enumerate(results_summary['recommendations'], 1):\n",
    "        print(f\"{i}. {rec['title']} ({rec['priority'].upper()})\")\n",
    "        print(f\"   {rec['description']}\")\n",
    "\n",
    "# Print key metrics\n",
    "print(\"\\nKEY FAIRNESS METRICS:\")\n",
    "for result in results_summary['detailed_metrics'][:5]:  # Show top 5\n",
    "    status = \"✓ PASS\" if result['passed'] else \"✗ FAIL\"\n",
    "    print(f\"{result['metric_name']}: {result['metric_value']:.3f} ({status})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ASSESSMENT COMPLETE\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}